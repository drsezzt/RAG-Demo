import os
import json
import faiss
import logging
import numpy as np
from datetime import datetime
import time as _time

from rag_app.vector_store.types import ChunkMeta, DocMap
from rag_app.core.interface import IVectorStore
from shared.config import get_vdb_config


logger = logging.getLogger("VDB")

class FaissVectorStore(IVectorStore):
    """
    FAISS å‘é‡åº“å­˜å‚¨å±‚
    è´Ÿè´£ï¼š
    - å‘é‡å­˜å–
    - ID æ˜ å°„
    - æŒä¹…åŒ–
    """

    def __init__(self):
        self.vdb_config = get_vdb_config()

        self.dim = self.vdb_config.dimension
        self.index_path = self.vdb_config.index_path
        self.map_path = self.vdb_config.map_path

        self.index = self._load_or_create_index()
        self.doc_map = self._load_or_create_map()

        # å¦‚æœ index / map ä»»ä¸€æŸåæˆ–ä¸ä¸€è‡´ï¼Œé‡ç½®ä»¥ä¿è¯å¯ç”¨æ€§
        # ï¼ˆæœ€å°ä¸€è‡´æ€§ï¼šä¸å› å•æ–‡ä»¶æŸåå¯¼è‡´æœåŠ¡èµ·ä¸æ¥ï¼‰
        if getattr(self.index, "ntotal", None) is not None and self.index.ntotal != self.doc_map.next_id:
            logger.warning(
                "op=vdb_store_inconsistent_reset "
                f"index_ntotal={self.index.ntotal} "
                f"map_next_id={self.doc_map.next_id}"
            )
            self._reset()

    # ============ åŠ è½½å‘é‡åº“ ============
    def _load_or_create_index(self):
        if os.path.exists(self.index_path):
            try:
                return faiss.read_index(self.index_path)
            except Exception:
                # index æ–‡ä»¶æŸåï¼šå¤‡ä»½å¹¶é‡å»º
                try:
                    bak = self.index_path + f".corrupt.{int(_time.time())}"
                    os.replace(self.index_path, bak)
                    logger.exception(f"op=faiss_index_corrupt_backup path={bak}")
                except Exception:
                    logger.exception("op=faiss_index_corrupt_backup_failed")
                return faiss.IndexFlatIP(self.dim)

        print("ğŸ†• Create new FAISS index")

        # ä½¿ç”¨æœ€åŸºç¡€ç‰ˆæœ¬ï¼ŒåæœŸå¯æ¢ IVF/HNSW
        index = faiss.IndexFlatIP(self.dim)     # å†…ç§¯
        # index = faiss.IndexFlatL2(self.dim)     # L2è·ç¦»

        return index

    # ============ åŠ è½½æ˜ å°„æ–‡ä»¶ ============
    def _load_or_create_map(self):
        if os.path.exists(self.map_path):
            try:
                with open(self.map_path, "r", encoding="utf-8") as f:
                    return DocMap.model_validate_json(f.read())
            except Exception:
                # JSON æˆªæ–­/æŸåï¼šå¤‡ä»½å¹¶é‡å»ºï¼ˆå¦åˆ™åº”ç”¨å¯åŠ¨ç›´æ¥å¤±è´¥ï¼‰
                try:
                    bak = self.map_path + f".corrupt.{int(_time.time())}"
                    os.replace(self.map_path, bak)
                    logger.exception(f"op=doc_map_corrupt_backup path={bak}")
                except Exception:
                    logger.exception("op=doc_map_corrupt_backup_failed")
                return DocMap()

        # é¦–æ¬¡åˆ›å»ºç«‹å³å†™ç›˜
        doc_map = DocMap()
        with open(self.map_path, "w", encoding="utf-8") as f:
            json.dump(doc_map.model_dump(), f, indent=2, ensure_ascii=False)

        return doc_map

    # ============ æŒä¹…åŒ–å‘é‡åº“ ============
    def _save(self):
        # å…ˆå†™ä¸´æ—¶æ–‡ä»¶å†åŸå­æ›¿æ¢ï¼Œé¿å…å´©æºƒå¯¼è‡´æ–‡ä»¶æˆªæ–­
        tmp_index_path = self.index_path + ".tmp"
        faiss.write_index(self.index, tmp_index_path)
        os.replace(tmp_index_path, self.index_path)

        tmp_map_path = self.map_path + ".tmp"
        with open(tmp_map_path, "w", encoding="utf-8") as f:
            # å…³é”®ï¼šä½¿ç”¨ pydantic çš„ JSON modeï¼Œç¡®ä¿ datetime è¢«åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
            f.write(self.doc_map.model_dump_json(indent=2))
        os.replace(tmp_map_path, self.map_path)

    # ============ å½’ä¸€åŒ–å¤„ç† ============
    def _normalize(self, vectors: np.ndarray):
        # å½’ä¸€åŒ–ï¼Œé€‚åˆ inner product æœç´¢
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1e-12
        return vectors / norms

    def get(self, chunk_id: int) -> ChunkMeta:
        """
        è·å–å‘é‡
        """

        return self.doc_map.chunks.get(chunk_id)

    # ============ æ·»åŠ å‘é‡ ============
    def add(
        self,
        metas: list[ChunkMeta],
        vectors: np.ndarray,
    ) -> bool:
        """
        æ·»åŠ å‘é‡

        vectors: (n, dim)
        """

        logger.info("op=chunk_add_start")
        if vectors.ndim != 2:
            raise ValueError("vectors must be 2D array")

        if vectors.shape[1] != self.dim:
            raise ValueError(f"dimension {vectors.shape[1]} mismatch {self.dim}")

        # å½’ä¸€åŒ–å¤„ç†
        vectors = self._normalize(vectors)

        count = vectors.shape[0]

        start_id = self.doc_map.next_id
        assert self.index.ntotal == start_id, "index size mismatch"

        ids = np.arange(start_id, start_id + count)
        self.index.add(vectors)

        # å»ºç«‹æ˜ å°„
        for i in ids:
            chunk = metas[i - start_id]
            chunk.chunk_id = int(i)
            self.doc_map.chunks[int(i)] = chunk

        self.doc_map.next_id += count

        self._save()
        logger.info("op=chunk_add_done")
        return True

    # ============ å‘é‡æ£€ç´¢ ============
    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10
    ) -> list[dict]:
        """
        æŸ¥è¯¢

        return:
        [
            {
              file_id,
              score,
              chunk_id
            }
        ]
        """

        logger.info("op=chunk_search_start")
        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)

        query_vector = self._normalize(query_vector)

        scores, ids = self.index.search(query_vector, top_k)

        results = []

        for score, idx in zip(scores[0], ids[0]):
            if idx == -1:
                continue

            meta = self.doc_map.chunks.get(idx)
            if not meta:
                continue

            results.append({
                "file_id": meta.file_id,
                "score": float(score),
                "chunk_id": int(idx),
            })
        logger.info(
            "op=chunk_search_done "
            f"results_count={len(results)}"
        )

        return results

    # ============ é€»è¾‘åˆ é™¤å‘é‡ï¼ˆé‡å»ºç´¢å¼•ï¼‰ ============
    def delete_by_file(
        self,
        file_id: str
    ) -> bool:
        """
        æ ¹æ® file_id åˆ é™¤
        FAISS ä¸æ”¯æŒç‰©ç†åˆ é™¤ â†’ é‡å»ºç´¢å¼•
        """

        logger.info(
            "op=chunk_delete_start "
            f"file_id={file_id}"
        )

        keep_ids = []
        for cid, meta in self.doc_map.chunks.items():
            if meta.file_id != file_id:
                keep_ids.append(cid)

        if not keep_ids:
            logger.warning("âš ï¸ No vectors to keep, reset index")
            self._reset()
            logger.info("op=chunk_del_empty")
            return True

        keep_ids = np.array(keep_ids)

        # å…¨é‡æ‹·è´ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½éšæ‚£ï¼Œéœ€è¦ä¼˜åŒ–
        vectors = self.index.reconstruct_n(0, self.index.ntotal)
        keep_vectors = vectors[keep_ids]

        # é‡å»ºç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dim)
        self.index.add(keep_vectors)

        # é‡å»º map
        new_chunks = {}
        for new_id, old_id in enumerate(keep_ids):
            meta = self.doc_map.chunks[old_id]
            meta.chunk_id = new_id
            new_chunks[new_id] = meta

        self.doc_map = DocMap(
            next_id=len(keep_ids),
            chunks=new_chunks
        )

        self._save()
        logger.info("op=chunk_delete_done")
        return True

    # ============ è·å–å‘é‡åº“ä¿¡æ¯ ============
    def info(self):
        return {
            "total_vectors": self.index.ntotal,
            "total_files": len(set(self.doc_map.chunks.values())),
        }

    # ============ é‡ç½®å‘é‡åº“ ============
    def _reset(self):
        self.index = faiss.IndexFlatIP(self.dim)

        self.doc_map = DocMap()

        self._save()
